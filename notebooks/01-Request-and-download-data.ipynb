{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d71717",
   "metadata": {},
   "source": [
    "### Import modules used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231381dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import io\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OOINet library\n",
    "sys.path.append(\"c:\\\\Users\\\\cooleyky\\\\Documents\\\\GitHub\\\\OOINet\") # this is what was missing from the steps I followed to install ooinet and ooi-data-explorations as local dev repo\n",
    "from ooinet import M2M\n",
    "from ooinet.Instrument.common import process_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9004d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from ooi-data-explorations library\n",
    "sys.path.append(\"c:\\\\Users\\\\cooleyky\\\\Documents\\\\GitHub\\\\ooi-data-explorations\\\\python\") # why did the initial install not include this?\n",
    "from ooi_data_explorations.uncabled.process_dosta import dosta_datalogger\n",
    "from ooi_data_explorations.combine_data import combine_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4949151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dask tools and ProgressBar\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef015859",
   "metadata": {},
   "source": [
    "### Define data parameters and routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86144467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters needed to request data\n",
    "refdes = \"CP01CNSM-MFD37-03-CTDBPD000\"              # Coastal Pioneer Array (NES) - Central Surface Mooring CTD Bottom-pumped\n",
    "method = \"recovered_inst\"                           # non-decimated data from recovered instrument\n",
    "stream = \"ctdbp_cdef_instrument_recovered\"          # name of data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf33ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic preprocessing routine to do some generic dataset cleaning/processing\n",
    "@dask.delayed\n",
    "def preprocess(ds):\n",
    "    ds = xr.open_dataset(ds)\n",
    "    ds = process_file(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c89b2",
   "metadata": {},
   "source": [
    "### QARTOD in Production: Request data from the THREDDS catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38249131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the gold copy THREDDs datasets\n",
    "thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "\n",
    "# Get the THREDDs catalog\n",
    "thredds_catalog = M2M.get_thredds_catalog(thredds_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the THREDDs catalog\n",
    "sensor_files, ancillary_files = M2M.clean_catalog(thredds_catalog, stream) \n",
    "# removes entries from thredds_catalog if they do not match the stream, or are used in processing data from the selected stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build the url to access the data\n",
    "sensor_files = [re.sub(\"catalog.html\\?dataset=\", M2M.URLS[\"goldCopy_dodsC\"], file) for file in sensor_files]\n",
    "# sensor_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12899b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "zs = [preprocess(file) for file in sensor_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf813cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets\n",
    "with ProgressBar():\n",
    "    data = xr.concat([ds.chunk() for ds in dask.compute(*zs)], dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data with a unique name\n",
    "ds_prod = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81992ff3",
   "metadata": {},
   "source": [
    "### QARTOD in Development: Request data from dev1 server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub in ooinet-dev1-west.intra.oceanobservatories.org into the avaialbe API urls\n",
    "Dev01_urls = {}\n",
    "for key in M2M.URLS:\n",
    "    url = M2M.URLS.get(key)\n",
    "    if \"opendap\" in url:\n",
    "        dev1_url = re.sub(\"opendap\", \"opendap-dev1-west.intra\", url)\n",
    "    else:\n",
    "        dev1_url = re.sub(\"ooinet\",\"ooinet-dev1-west.intra\", url)\n",
    "    Dev01_urls[key] = dev1_url\n",
    "    \n",
    "# Use the gold copy THREDDs datasets\n",
    "thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True) # in this example we recycle refdes, method, stream\n",
    "\n",
    "# Get the THREDDs catalog\n",
    "thredds_catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "# Clean the THREDDs catalog\n",
    "sensor_files, ancillary_files = M2M.clean_catalog(thredds_catalog, stream)\n",
    "\n",
    "# Now build the url to access the data\n",
    "sensor_files = [re.sub(\"catalog.html\\?dataset=\", M2M.URLS[\"goldCopy_dodsC\"], file) for file in sensor_files]\n",
    "zs = [preprocess(file) for file in sensor_files]\n",
    "\n",
    "# Load all the datasets\n",
    "with ProgressBar():\n",
    "    data = xr.concat([ds.chunk() for ds in dask.compute(*zs)], dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data with a unique name\n",
    "ds_dev = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf34610",
   "metadata": {},
   "source": [
    "### Save datasets to interim data folder for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# something along the lines of write_nc(), xr.save_dataset?\n",
    "# call the data file datasets_for_testing.nc \n",
    "# I think we will test datasets in production and development separately, so maybe making separate files for those types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
